{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up a GPU Deep Learning Server\n",
    "Most of the work you end up doing with *most* frameworks will require a GPU. Nvidia is the way to go, but I will be doing this course using WSL 2.0, and DirectML to use the Discrete Intel Graphics on this machine.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not have the time/need to set up on a machine, Google Collaboratory is a good bet, as you can get access to a good GPU, and have everything hosted on a server (therefore not requiring great specs)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the first example, as given in the book. All of this is trained on-device, as I prefer it over using an online service. I got good results with WSL 2.0, paired with DirectML, which allowed for using both the CPU and GPU on this machine (of which is not Nvidia), and also maximise the RAM that it used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "\n",
      "NVIDIA GeForce GTX 950\n",
      "Memory Usage:\n",
      "Allocated: 0.1 GB\n",
      "Cached:    0.9 GB\n",
      "|===========================================================================|\n",
      "|                  PyTorch CUDA memory summary, device ID 0                 |\n",
      "|---------------------------------------------------------------------------|\n",
      "|            CUDA OOMs: 7            |        cudaMalloc retries: 7         |\n",
      "|===========================================================================|\n",
      "|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocated memory      |  133249 KB |     953 MB |    3575 MB |    3445 MB |\n",
      "|       from large pool |  127168 KB |     937 MB |    3529 MB |    3404 MB |\n",
      "|       from small pool |    6081 KB |      15 MB |      46 MB |      40 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active memory         |  133249 KB |     953 MB |    3575 MB |    3445 MB |\n",
      "|       from large pool |  127168 KB |     937 MB |    3529 MB |    3404 MB |\n",
      "|       from small pool |    6081 KB |      15 MB |      46 MB |      40 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved memory   |     956 MB |     978 MB |    1344 MB |  397312 KB |\n",
      "|       from large pool |     942 MB |     962 MB |    1324 MB |  391168 KB |\n",
      "|       from small pool |      14 MB |      16 MB |      20 MB |    6144 KB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable memory |  622462 KB |  622462 KB |    3046 MB |    2438 MB |\n",
      "|       from large pool |  614208 KB |  614208 KB |    2987 MB |    2387 MB |\n",
      "|       from small pool |    8254 KB |   12206 KB |      59 MB |      50 MB |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Allocations           |     233    |     558    |    1560    |    1327    |\n",
      "|       from large pool |      21    |      48    |     154    |     133    |\n",
      "|       from small pool |     212    |     510    |    1406    |    1194    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Active allocs         |     233    |     558    |    1560    |    1327    |\n",
      "|       from large pool |      21    |      48    |     154    |     133    |\n",
      "|       from small pool |     212    |     510    |    1406    |    1194    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| GPU reserved segments |      19    |      20    |      29    |      10    |\n",
      "|       from large pool |      12    |      12    |      19    |       7    |\n",
      "|       from small pool |       7    |       8    |      10    |       3    |\n",
      "|---------------------------------------------------------------------------|\n",
      "| Non-releasable allocs |      26    |      59    |     312    |     286    |\n",
      "|       from large pool |      13    |      13    |      57    |      44    |\n",
      "|       from small pool |      13    |      50    |     255    |     242    |\n",
      "|===========================================================================|\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>1.129659</td>\n",
       "      <td>0.812928</td>\n",
       "      <td>0.337618</td>\n",
       "      <td>00:35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>error_rate</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.826665</td>\n",
       "      <td>0.753736</td>\n",
       "      <td>0.331529</td>\n",
       "      <td>00:13</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastai.vision.all import * \n",
    "path = untar_data(URLs.PETS)/'images'\n",
    "import torch\n",
    "torch.cuda.empty_cache()\n",
    "import gc\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "def is_cat(x): return x[0].isupper() \n",
    "dls = ImageDataLoaders.from_name_func( path, get_image_files(path), valid_pct=0.2, seed=42, label_func=is_cat, item_tfms=Resize(4))\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using device:', device)\n",
    "print()\n",
    "\n",
    "#Additional Info when using cuda\n",
    "if device.type == 'cuda':\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "    print('Memory Usage:')\n",
    "    print('Allocated:', round(torch.cuda.memory_allocated(0)/1024**3,1), 'GB')\n",
    "    print('Cached:   ', round(torch.cuda.memory_reserved(0)/1024**3,1), 'GB')\n",
    "\n",
    "print(torch.cuda.memory_summary(device=None, abbreviated=False))\n",
    "\n",
    "learn = cnn_learner(dls, resnet34, metrics=error_rate)\n",
    "learn.fine_tune(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce GTX 950'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pycuda.driver as cuda\n",
    "cuda.init()\n",
    "## Get Id of default device\n",
    "torch.cuda.current_device()\n",
    "# 0\n",
    "cuda.Device(0).name() # '0' is the id of your GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are able to use the 'error' rate column in the table above to look at the accuracy of the model. This is vital in the building of deep learning models, as it shows how reliable and accurate it can be."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the image I will be using to judge if this model is accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<img src=\"Images/cat.jpg\"/>"
      ],
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Image\n",
    "Image(url= \"Images/cat.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the end of the first classifier model from within the book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Machine Learning?\n",
    "\n",
    "This classifier that was made is a deep learning model. Deep Learning models use neural networks, which were initially produced in the 1950s, and are an incredibly versatile and powerful method of Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Learning is a more modern and complex form of Machine Learning, and I will write down some more notes from the book in the following sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Machine Learning* is similar to normal programming by the fact that is is a way to get computers to complete a specific task. The main difference is that it avoids the programmer from having to account for every individual case manually, and allows for the computer to do it itself, and therefore be more efficient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right at the beginning of computing, all the way back in 1949, an IBM researcher (Arthur Samuel) worked on a different way of getting machines to complete tasks, which he coined *'Machine Learning'*.\n",
    "\n",
    "- *\"Programming a computer for such computations is, at best, a difficult task, not primarily because of any inherent complexity in the computer itself but, rather, because of the need to spell out every minute step of the process in the most exasperating detail. Computers, as any programmer will tell you, are giant morons, not\n",
    "giant brains\"*\n",
    "- *\"Suppose we arrange for some automatic means of testing the effectiveness of any current weight assignment in terms of actual performance and provide a mechanism for altering the weight assignment so as to maximize the performance. We need not go into the details of such a procedure to see that it could be made entirely automatic and to see that a machine so programmed would\n",
    "“learn” from its experience.\"*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of powerful concepts embedded in this short statement:\n",
    "- The idea of a “weight assignment”\n",
    "- The fact that every weight assignment has some “actual performance”\n",
    "- The requirement that there be an “automatic means” of testing that performance\n",
    "- The need for a “mechanism” (i.e., another automatic process) for improving the performance by changing the\n",
    "weight assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rundown of the above code\n",
    "Here I'll go through the above code line by line and explain what is happening"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai.vision.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this line, I am importing all of the libraries that are used from within the fastai.vision library. This gives a lot of functions and classes which will be used to create a wide variety of models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is often advised not to import all sections of a framework or library into an environment (i.e. importing with ``import *``), although this library has been optimised to be used like this, so it isn't much of an issue, and can make things more convenient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = untar_data(URLs.PETS)/'images'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This line downloads a standard data set from the 'fast-ai datasets' collection (https://course.fast.ai/datasets), and returns it to the variable ``path``"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "bc033ac1d5b396efcb8d2b9c8ebd08a6468f4056aed3d5e1ac94427d17274a41"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('pydml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
